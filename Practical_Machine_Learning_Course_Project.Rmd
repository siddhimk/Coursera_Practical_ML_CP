---
title: "Practical Machine Learning Course Project"
author: "SMK"
date: "11/26/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

This document is created to summarize the results of Practical Machine Learning Course Project. Different machine learning models were created to identify how well the participants performed the barbell lifts. The goal of this project was to predict the manner in which they did the exercise. The analysis was done using the machine learning algorithms such as Decision Tree, Support Vector Machine, Random Forest, Gradient Boosted Trees using k-fold cross-validation on the training data set. It has been observed that the Random Forest Algorithm gave the highest accuracy based on the training and validations datasets. Hence, this model is being used to predict the performance of the test dataset.  


## Reading Dataset

The data for this exercise was collected from from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

```{r}
library(caret)
library(ggplot2)
library(lattice)
library(kernlab)
library(rattle)
library(corrplot)

set.seed(1234)

url_train <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

df_training <- read.csv(url(url_train))
df_testing <- read.csv(url(url_test))

# Let's check whether the datasets are properly loaded or not

dim(df_training)
dim(df_testing)
```

The given training and test data sets includes 160 variables. Also, there are 19,622 observations in the training set and 20 observations in the test set. 


## Data Pre-Processing

Next step after reading the data is for data pre-processing. This is an important step before adding data into the model in order to get the appropriate test results. 

Different steps can be performed to cleanup the given data such as removing N/A variables, metadata columns, zero variance variables or constants.

```{r}
# Removing columns with most N/As
df_training <- df_training[, colMeans(is.na(df_training)) < 0.9]

# Removing metadata columns
df_training <- df_training[, -c(1:7)]

# Removing near zero variance variables 
nzv <- nearZeroVar(df_training)
df_training <- df_training[, -nzv]
dim(df_training)
```


## Data Preperation for Data Modeling Exercise

After data pre-processing is done, next step is to divide the data into Training dataset and Validation dataset. This step prepares the dataset so that they can be passed to the Machine Learning Algorithms. 

```{r}
# Divide the training dataset into train and validation dataset 

div_training <- createDataPartition(y=df_training$classe, p=0.7, list=F)
df_train <- df_training[div_training,]
df_valid <- df_training[-div_training,]
```

Let's setup the control for using 3-fold cross valdation. 

```{r}
df_cntrl <- trainControl(method = "cv", number = 3, verboseIter = F)
```


## Machine Learning Algorithms

### Decision Tree

Let's start with the Decision Tree model and analyze it's performance. 

```{r}
dec_tree <- train(classe~., data=df_train, method="rpart", trControl=df_cntrl, tuneLength=5)

pred_dec_tree <- predict(dec_tree, df_valid)

eval_dec_tree <- confusionMatrix(pred_dec_tree, factor(df_valid$classe))

eval_dec_tree
```

### Random Forest

Let's build the Random Forest model next and analyze it's performance.

```{r}
rnd_frst <- train(classe~., data=df_train, method="rf", trControl=df_cntrl, tuneLength=5)

pref_rnd_frst <- predict(rnd_frst, df_valid)

eval_rnd_frst <- confusionMatrix(pref_rnd_frst, factor(df_valid$classe))

eval_rnd_frst
```

### Gradient Boosted Trees

Let's now build the Gradient Boosted Trees model and analyze it's performance. 

```{r}
grd_bst <- train(classe~., data=df_train, method="gbm", trControl=df_cntrl, tuneLength=5, verbose=F)

pred_grd_bst <- predict(grd_bst, df_valid)

eval_grd_bst <- confusionMatrix(pred_grd_bst, factor(df_valid$classe))

eval_grd_bst
```

### SVM

```{r}
svm <- train(classe~., data=df_train, method="svmLinear", trControl=df_cntrl, tuneLength=5, verbose=F)

pred_svm <- predict(svm, df_valid)

eval_svm <- confusionMatrix(pred_svm, factor(df_valid$classe))

eval_svm
```


## Model Selection 

Let's check the accuracy and out of sample error rate for each of the above generated model. 

```{r}
mdls <- c("Decision Tree", "Random Forest", "GBM", "SVM")
acc <- round(c(eval_dec_tree$overall[1], eval_rnd_frst$overall[1], eval_grd_bst$overall[1], eval_svm$overall[1]), 3)
err <- 1 - acc
data.frame(Accuracy=acc, Error=err, row.names=mdls)
```

After running different machine learning models and analyzing performance of these in terms of accuracy, Random Forest model gives the highest accuracy 0.9954 and 0.0046 out of sample error rate


## Conclusion

The Random Forest model resulted into highest accuracy and least out of sample error rate. Hence Random Forest algorithm is selected for modeling the given test set. 

```{r}
pred_test <- predict(rnd_frst, df_testing) 

pred_test
```


## Appendix

### Figure 1: Correlation metrics of variables in the training dataset

```{r}
plt_corr <- cor(df_train[, -length(names(df_train))])
corrplot(plt_corr, method = "color")
```

### Figure 2: Decision Tree plot

```{r}
fancyRpartPlot(dec_tree$finalModel)
```

### Figure 3: Random Forest plot

```{r}
plot(rnd_frst)
```

### Figure 4: Gradient Boosted Trees plot

```{r}
plot(grd_bst)
```

